{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Machine Translation",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suneetsawant/Neural-Machine-Translation/blob/master/Neural_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "quiFlxb4hWJO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation Using Attention \n",
        "\n",
        "This notebook covers implementation of converting English to  French using  Neural Network incorporated with attention mechanism .\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tyF7ka4WjWin",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import all neccessary libraries \n",
        "\n",
        "1. Tensorflow, Keras for implementing neural network architecture \n",
        "2. Pydrive to save-restore model on google drive \n",
        "3. NLTK to calculate BLEU Scores "
      ]
    },
    {
      "metadata": {
        "id": "Iag0EDGQ_i-m",
        "colab_type": "code",
        "outputId": "cb71b533-6080-4662-ef90-1188b535df5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U nltk   ## Updated nltk library for computing BLEU Scores\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        " \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import re\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Dense,GRU,Embedding,GlobalMaxPooling2D,Dropout,add,CuDNNGRU,Activation\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply,RepeatVector,Lambda,Reshape\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "import keras as K\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import unicodedata\n",
        "import sys\n",
        "import gc\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import nltk\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "!pip install -U -q PyDrive     ##### For saving-downloading model to google drive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "class Drive():   \n",
        "  def __init__(self) :  \n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    self.drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "  def fileaction(self,files,op='up') : \n",
        "    file_list = self.drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
        "    for filename in files:\n",
        "      flag = 0 \n",
        "\n",
        "      for file1 in file_list:\n",
        "        if (file1['title']) == filename :\n",
        "            if (op == 'up'and flag==0) : \n",
        "                file1.Delete()\n",
        "                self.upload(filename)\n",
        "                flag = 1\n",
        "\n",
        "            elif (op == 'down') : \n",
        "                self.download(filename,file1) \n",
        "\n",
        "      if(op=='up' and flag==0): \n",
        "            self.upload(filename)\n",
        "            flag = 1\n",
        "\n",
        "  def upload(self,filename) : \n",
        "      Uploadfile = self.drive.CreateFile({'title': filename})\n",
        "      Uploadfile.SetContentFile(filename)\n",
        "      Uploadfile.Upload()\n",
        "      print(\"Saved '{}' to Drive\".format(filename))\n",
        "\n",
        "  def download(self,filename,file1): \n",
        "      downloaded = self.drive.CreateFile({'id':file1['id']})\n",
        "      downloaded.GetContentFile(filename)\n",
        "      print(\"Downloaded '{}' from Drive\".format(filename))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\n",
            "\r\u001b[K    0% |▎                               | 10kB 17.9MB/s eta 0:00:01\r\u001b[K    1% |▌                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K    2% |▊                               | 30kB 2.5MB/s eta 0:00:01\r\u001b[K    2% |█                               | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    3% |█▏                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    4% |█▍                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 71kB 2.9MB/s eta 0:00:01\r\u001b[K    5% |█▉                              | 81kB 3.3MB/s eta 0:00:01\r\u001b[K    6% |██                              | 92kB 3.6MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 102kB 2.8MB/s eta 0:00:01\r\u001b[K    7% |██▌                             | 112kB 2.8MB/s eta 0:00:01\r\u001b[K    8% |██▊                             | 122kB 4.0MB/s eta 0:00:01\r\u001b[K    9% |███                             | 133kB 4.0MB/s eta 0:00:01\r\u001b[K    10% |███▏                            | 143kB 7.5MB/s eta 0:00:01\r\u001b[K    10% |███▍                            | 153kB 7.5MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 163kB 7.4MB/s eta 0:00:01\r\u001b[K    12% |████                            | 174kB 7.4MB/s eta 0:00:01\r\u001b[K    12% |████▏                           | 184kB 7.4MB/s eta 0:00:01\r\u001b[K    13% |████▍                           | 194kB 7.4MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 204kB 36.2MB/s eta 0:00:01\r\u001b[K    15% |████▉                           | 215kB 8.4MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 225kB 8.4MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 235kB 8.4MB/s eta 0:00:01\r\u001b[K    17% |█████▌                          | 245kB 8.5MB/s eta 0:00:01\r\u001b[K    17% |█████▊                          | 256kB 8.5MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 266kB 8.4MB/s eta 0:00:01\r\u001b[K    19% |██████▏                         | 276kB 8.6MB/s eta 0:00:01\r\u001b[K    20% |██████▍                         | 286kB 8.7MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 296kB 8.6MB/s eta 0:00:01\r\u001b[K    21% |██████▉                         | 307kB 8.9MB/s eta 0:00:01\r\u001b[K    22% |███████                         | 317kB 49.6MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 327kB 52.3MB/s eta 0:00:01\r\u001b[K    23% |███████▌                        | 337kB 54.8MB/s eta 0:00:01\r\u001b[K    24% |███████▉                        | 348kB 49.0MB/s eta 0:00:01\r\u001b[K    25% |████████                        | 358kB 48.5MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 368kB 53.2MB/s eta 0:00:01\r\u001b[K    26% |████████▌                       | 378kB 53.1MB/s eta 0:00:01\r\u001b[K    27% |████████▊                       | 389kB 55.0MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 399kB 9.4MB/s eta 0:00:01\r\u001b[K    28% |█████████▏                      | 409kB 9.4MB/s eta 0:00:01\r\u001b[K    29% |█████████▍                      | 419kB 9.4MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 430kB 9.3MB/s eta 0:00:01\r\u001b[K    30% |█████████▉                      | 440kB 9.3MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 450kB 9.4MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 460kB 9.3MB/s eta 0:00:01\r\u001b[K    32% |██████████▌                     | 471kB 9.3MB/s eta 0:00:01\r\u001b[K    33% |██████████▊                     | 481kB 9.3MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 491kB 9.3MB/s eta 0:00:01\r\u001b[K    35% |███████████▏                    | 501kB 50.6MB/s eta 0:00:01\r\u001b[K    35% |███████████▍                    | 512kB 46.1MB/s eta 0:00:01\r\u001b[K    36% |███████████▊                    | 522kB 47.2MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 532kB 48.5MB/s eta 0:00:01\r\u001b[K    37% |████████████▏                   | 542kB 48.4MB/s eta 0:00:01\r\u001b[K    38% |████████████▍                   | 552kB 52.4MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 563kB 54.8MB/s eta 0:00:01\r\u001b[K    40% |████████████▉                   | 573kB 55.1MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 583kB 57.0MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 593kB 56.5MB/s eta 0:00:01\r\u001b[K    42% |█████████████▌                  | 604kB 56.5MB/s eta 0:00:01\r\u001b[K    42% |█████████████▊                  | 614kB 63.7MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 624kB 61.5MB/s eta 0:00:01\r\u001b[K    44% |██████████████▏                 | 634kB 58.8MB/s eta 0:00:01\r\u001b[K    45% |██████████████▍                 | 645kB 58.6MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 655kB 56.7MB/s eta 0:00:01\r\u001b[K    46% |██████████████▉                 | 665kB 46.5MB/s eta 0:00:01\r\u001b[K    47% |███████████████                 | 675kB 46.4MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 686kB 46.3MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 696kB 46.3MB/s eta 0:00:01\r\u001b[K    49% |███████████████▉                | 706kB 44.7MB/s eta 0:00:01\r\u001b[K    50% |████████████████                | 716kB 45.8MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 727kB 46.4MB/s eta 0:00:01\r\u001b[K    51% |████████████████▌               | 737kB 48.4MB/s eta 0:00:01\r\u001b[K    52% |████████████████▊               | 747kB 49.2MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 757kB 49.8MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▏              | 768kB 62.3MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▍              | 778kB 62.1MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▋              | 788kB 60.8MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▉              | 798kB 61.9MB/s eta 0:00:01\r\u001b[K    56% |██████████████████              | 808kB 16.5MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 819kB 16.3MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▌             | 829kB 16.3MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▊             | 839kB 16.4MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 849kB 16.4MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▏            | 860kB 15.9MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▌            | 870kB 15.6MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▊            | 880kB 15.6MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 890kB 15.7MB/s eta 0:00:01\r\u001b[K    62% |████████████████████▏           | 901kB 15.7MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▍           | 911kB 54.2MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 921kB 56.1MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 931kB 56.8MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 942kB 56.9MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 952kB 56.6MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▌          | 962kB 64.3MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▊          | 972kB 68.3MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 983kB 69.4MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▏         | 993kB 69.8MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▍         | 1.0MB 70.5MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 1.0MB 70.2MB/s eta 0:00:01\r\u001b[K    71% |██████████████████████▉         | 1.0MB 69.9MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▏        | 1.0MB 69.1MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▍        | 1.0MB 69.6MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 1.1MB 70.2MB/s eta 0:00:01\r\u001b[K    74% |███████████████████████▉        | 1.1MB 64.8MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████        | 1.1MB 60.7MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 1.1MB 61.0MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▌       | 1.1MB 50.5MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▊       | 1.1MB 50.2MB/s eta 0:00:01\r\u001b[K    77% |█████████████████████████       | 1.1MB 49.6MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 1.1MB 49.9MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▍      | 1.1MB 48.3MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▋      | 1.1MB 47.5MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 1.2MB 47.4MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████      | 1.2MB 12.4MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▎     | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▊     | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▎    | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▌    | 1.2MB 13.0MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▊    | 1.2MB 13.1MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████    | 1.2MB 13.1MB/s eta 0:00:01\r\u001b[K    87% |████████████████████████████▏   | 1.3MB 13.0MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▍   | 1.3MB 59.7MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▋   | 1.3MB 58.3MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 1.3MB 59.4MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████   | 1.3MB 59.8MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▎  | 1.3MB 58.9MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 1.3MB 59.9MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▊  | 1.3MB 59.9MB/s eta 0:00:01\r\u001b[K    93% |██████████████████████████████  | 1.3MB 58.5MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 1.4MB 60.1MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▍ | 1.4MB 59.6MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▋ | 1.4MB 61.5MB/s eta 0:00:01\r\u001b[K    96% |███████████████████████████████ | 1.4MB 62.6MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 1.4MB 61.9MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▍| 1.4MB 40.0MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▋| 1.4MB 39.7MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 1.4MB 39.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 1.4MB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
            "Collecting singledispatch (from nltk)\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/10/369f50bcd4621b263927b0a1519987a04383d4a98fb10438042ad410cf88/singledispatch-3.4.0.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/4b/c8/24/b2343664bcceb7147efeb21c0b23703a05b23fcfeaceaa2a1e\n",
            "Successfully built nltk\n",
            "Installing collected packages: singledispatch, nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4 singledispatch-3.4.0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 19.5MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 1.5MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.6MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 1.9MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.3MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 2.7MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 3.0MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.4MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.6MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.6MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 3.9MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 3.8MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 7.2MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 7.2MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 7.0MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 6.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 6.6MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 6.5MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 21.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 8.2MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 8.2MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 8.3MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 8.3MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 8.4MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 8.4MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 9.0MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 9.2MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 9.5MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 10.0MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 45.2MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 45.3MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 46.4MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 42.4MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 40.3MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 45.8MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 45.9MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 46.0MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 9.1MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 9.1MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 8.9MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 8.9MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 8.9MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 9.0MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 9.0MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 9.0MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 9.0MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 9.0MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 42.9MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 41.5MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 44.8MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 47.0MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 47.8MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 50.9MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 51.1MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 49.2MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 49.9MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 49.8MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 50.7MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 54.8MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 54.2MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 53.0MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 51.7MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 49.6MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 37.8MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 11.4MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 11.3MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 11.3MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 11.3MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 11.3MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 11.2MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 11.2MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 11.2MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 11.3MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 12.2MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 47.9MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 49.2MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 50.6MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 50.8MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 50.9MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 52.8MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 52.3MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 53.0MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 49.5MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 48.9MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 50.1MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 51.1MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 50.9MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 51.3MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 50.4MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 50.9MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 52.1MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 51.2MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 55.6MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 56.4MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 56.0MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.6MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qYPIT5A8lQT9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download Data \n",
        "\n",
        "The dataset for training can be found [here](https://download.pytorch.org/tutorial/data.zip). The trick mentioned in [fast ai forums](https://forums.fast.ai/t/request-curl-from-chrome/6816/7) has been used to download the data directly to colab instance."
      ]
    },
    {
      "metadata": {
        "id": "_UzfhFaQmFaw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget --header=\"Host: download.pytorch.org\" --header=\"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\" --header=\"Cookie: _ga=GA1.2.1452943855.1552129267; _gid=GA1.2.24346264.1552129267\" --header=\"Connection: keep-alive\" \"https://download.pytorch.org/tutorial/data.zip\" -O \"data.zip\" -c\n",
        "!unzip data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BUyAWy-dnFSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PreProcess Downloaded Data\n",
        "1.  Adding start and end of the sentence tags\n",
        "2. Creating english and french tokenizers \n",
        "3. Dividing the data into training, test, validation sets \n",
        "4. Saving all the data(with divisions) as dictionary to drive for future use.  \n",
        "\n",
        "Note: This is all one time process. The data will be downloaded for training whenever required. This helps us in maintaining the same training and validation data for training as the dataset divisions are randomized."
      ]
    },
    {
      "metadata": {
        "id": "cuFcq-dpq9UD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Helper functions for preprocessing \n",
        "def process_Data(data,sequence_length):\n",
        "        tokenizer = Tokenizer(oov_token='UNK',num_words = 1e4)\n",
        "        tokenizer.fit_on_texts(data) \n",
        "        data_clean = tokenizer.texts_to_sequences(data)\n",
        "        data_clean = pad_sequences(data_clean,sequence_length,padding='post')\n",
        "\n",
        "        return data_clean,tokenizer\n",
        "  \n",
        "def read_data(filename):\n",
        "  with open(filename, mode='rt', encoding='utf-8') as file:  \n",
        "          dataF = file.read() \n",
        "          dataF = dataF.strip().split('\\n')\n",
        "          eng = []\n",
        "          french = []\n",
        "          for sentence in dataF: \n",
        "              split_sentence = sentence.strip().split('\\t')\n",
        "              sentence = \"strr \" + normalizeString(split_sentence[0])+ \" endd\"\n",
        "              sentence = re.sub('\\d+','num',sentence) \n",
        "\n",
        "              eng.append(sentence)\n",
        "\n",
        "              sentence = \"strr \" + normalizeString(split_sentence[1])+ \" endd\"\n",
        "              sentence = re.sub('\\d+','num',sentence) \n",
        "\n",
        "              french.append(sentence) \n",
        "  return eng,french       \n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    \n",
        "    return s   \n",
        "\n",
        "  \n",
        "###----------------------Pre-Processing Data ----------------------------------------#####\n",
        "\n",
        "\n",
        "sequence_length = 40\n",
        "english_Data,french_Data = read_data(\"data/eng-fra.txt\")\n",
        "eng_data, eng_tokenizer = process_Data(english_Data,sequence_length)\n",
        "french_data, french_tokenizer = process_Data(french_Data,sequence_length)\n",
        "\n",
        "\n",
        "data = {}\n",
        "data['eng_tokenizer']    = eng_tokenizer\n",
        "data['french_tokenizer'] = french_tokenizer\n",
        "\n",
        "x = len(eng_data)\n",
        "train_idx = np.array(np.random.choice(np.arange(x),int(0.8*x),replace=False))\n",
        "val_idx = np.array(np.random.choice(list(set(np.arange(x))-set(train_idx)),int(0.1*x),replace=False)) \n",
        "test_idx = list(set(np.arange(x))-set(train_idx)-set(val_idx))\n",
        "print(train_idx,val_idx,test_idx)\n",
        "\n",
        "\n",
        "data['eng_data_train'] = eng_data[train_idx]\n",
        "data['eng_data_val'] = eng_data[val_idx]\n",
        "data['eng_data_test'] = eng_data[test_idx]  \n",
        "\n",
        "data['french_data_train'] = french_data[train_idx]\n",
        "data['french_data_val'] = french_data[val_idx]\n",
        "data['french_data_test'] = french_data[test_idx]  \n",
        "\n",
        "\n",
        "with open(\"data.pkl\", 'wb') as f:  \n",
        "        pickle.dump(data, f)\n",
        "!ls\n",
        "Drive().fileaction(['data.pkl'],'up')  ## Upload the preprocess Data to drive \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IslCkcKo1ok-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training "
      ]
    },
    {
      "metadata": {
        "id": "7IehMkKk14C_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Encoder and Decoder Model"
      ]
    },
    {
      "metadata": {
        "id": "ig6Wqet__S7D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderModel(tf.keras.Model):\n",
        "  def __init__(self,batch_size,embedding_dim,vocab_size,encoder_units):\n",
        "    super(EncoderModel, self).__init__()\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.encoder_units = encoder_units\n",
        "    \n",
        "    self.Embedding = Embedding(self.vocab_size,self.embedding_dim,input_length=40) \n",
        "    self.GRU = CuDNNGRU(encoder_units,return_sequences = True,return_state= True) \n",
        "\n",
        "  def call(self, inp):\n",
        "    \n",
        "    embedded_vector = self.Embedding(inp)\n",
        "    output,hidden_state = self.GRU(embedded_vector)\n",
        "    return output,hidden_state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_size, self.encoder_units))\n",
        "    \n",
        "### ----------------- Decoder Model ---------------------------------------------#####\n",
        "\n",
        "\n",
        "class DecoderModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self,batch_size,embedding_dim,vocab_size,decoder_units):\n",
        "    super(DecoderModel, self).__init__()\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.decoder_units = decoder_units\n",
        "    \n",
        "    \n",
        "    self.W1 = Dense(self.decoder_units)\n",
        "    self.W2 = Dense(self.decoder_units)\n",
        "    self.V  = Dense(1)\n",
        "    self.A = Dense(self.decoder_units,activation='tanh')\n",
        "\n",
        "    \n",
        "    self.Embedding = Embedding(self.vocab_size,self.embedding_dim) \n",
        "    self.GRU = CuDNNGRU(self.decoder_units,return_sequences = True,return_state= True) \n",
        "    self.FC = Dense(self.vocab_size)\n",
        "    \n",
        "  def call(self,decoder_input,hidden,enc_hidden_output):\n",
        "    \n",
        "    decoder_hidden = tf.expand_dims(hidden,axis=1)                                  #output_shape = (batch_size,1,hidden_size)\n",
        "    score = self.V(tf.nn.tanh(self.W1(decoder_hidden)+self.W2(enc_hidden_output)))  #output_shape = (batch_size,sequence_length,1)\n",
        "    \n",
        "    attention_weights = tf.nn.softmax(score,axis=1)    #output_shape = (batch_size,sequence_length,1)\n",
        "    \n",
        "    context = attention_weights * enc_hidden_output\n",
        "    context = tf.reduce_sum(context,axis=1)            #output_shape = (batch_size,hidden_size)\n",
        "    context = tf.expand_dims(context,axis=1)\n",
        "    \n",
        "    decoder_embedded_vector = self.Embedding(decoder_input) \n",
        "    \n",
        "    attention_vector = self.A(tf.concat([context,decoder_embedded_vector],axis=-1))\n",
        "    \n",
        "    out,state = self.GRU(attention_vector)\n",
        "    \n",
        "    out = tf.squeeze(out,axis=1)\n",
        "    \n",
        "    output = self.FC(out)\n",
        "    \n",
        "    return output, state, attention_weights\n",
        "    \n",
        "     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYOBU2iYAW06",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download Processed data for training "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "x8xLIHU112_O",
        "outputId": "3e49119b-a132-4966-a61f-bc8836746aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "Drive().fileaction(['data.pkl'],'down')\n",
        "\n",
        "with open(\"data.pkl\", 'rb') as f:  \n",
        "        data = pickle.load( f)\n",
        "    \n",
        "    \n",
        "train_index = 500000\n",
        "val_index = 20000\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((data['eng_data_train'],data['french_data_train']))\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((data['eng_data_val'],data['french_data_val']))\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloaded 'data.pkl' from Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1l4DMlnfAy5u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Parameters and Instantiate Models\n"
      ]
    },
    {
      "metadata": {
        "id": "xj2HI8HTMKX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sequence_length    = 40\n",
        "embedding_dim      = 128 \n",
        "batch_size         = batch_size\n",
        "eng_vocab_size = 15000\n",
        "french_vocab_size = 15000\n",
        "epochs = 5\n",
        "encoder_units = 256\n",
        "decoder_units = 256 \n",
        "N_BATCH = data['eng_data_train'].shape[0] // batch_size\n",
        "N_VAl = data['eng_data_val'].shape[0] // batch_size\n",
        "\n",
        "learning_rate = tf.Variable(0.0008)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
        "\n",
        "encoder = EncoderModel(batch_size,embedding_dim,eng_vocab_size,encoder_units)\n",
        "decoder = DecoderModel(batch_size,embedding_dim,french_vocab_size,decoder_units)\n",
        "\n",
        "\n",
        "def compute_loss(real, pred):\n",
        "  mask = 1 - np.equal(real, 0)\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "  return tf.reduce_mean(loss)\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,decoder=decoder)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zIZYbUo935b8",
        "colab_type": "code",
        "outputId": "99977c63-6499-4ec8-e56f-2346e61a5ea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "## Download the saved model\n",
        "!rm -r training_checkpoints\n",
        "\n",
        "Drive().fileaction(['model.zip'],'down')\n",
        "!unzip -q model.zip\n",
        "#!unzip -q model.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'training_checkpoints': No such file or directory\n",
            "Downloaded 'model.zip' from Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TofWYeWeaXl8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Cyclic LR \n",
        "\n",
        "For efficient tuning of learning rate,  we are using  [Cyclical Learning Rate Policy](https://arxiv.org/abs/1506.01186)"
      ]
    },
    {
      "metadata": {
        "id": "5hMzcjCJbH7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cyclic_lr(base_lr,max_lr,clr_iterations,mode,step_size,scale_fn): \n",
        "    if scale_fn == None:\n",
        "            if mode == 'triangular':\n",
        "                scale_fn = lambda x: 1.\n",
        "                scale_mode = 'cycle'\n",
        "            elif mode == 'triangular2':\n",
        "                scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                scale_mode = 'cycle'\n",
        "            elif mode == 'exp_range':\n",
        "                scale_fn = lambda x: gamma**(x)\n",
        "                scale_mode = 'iterations'\n",
        "    else:\n",
        "            scale_fn = scale_fn\n",
        "    cycle = np.floor(1+clr_iterations/(2*step_size))\n",
        "    x = np.abs(clr_iterations/step_size - 2*cycle + 1)\n",
        "    if scale_mode == 'cycle':\n",
        "       return base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))*scale_fn(cycle)\n",
        "    else:\n",
        "       return base_lr + (max_lr-base_lr)*np.maximum(0, (1-x))*scale_fn(clr_iterations)   \n",
        "    \n",
        "\n",
        "lr = []\n",
        "for i in range(100000): \n",
        "  lr.append(cyclic_lr(1e-5,4.5e-5,i,'triangular',10000,None))\n",
        "\n",
        "\n",
        "#plt.plot(lr)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NnflXLC4Wauh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training Loop \n"
      ]
    },
    {
      "metadata": {
        "id": "xDIQqD5gTCCO",
        "colab_type": "code",
        "outputId": "4f58ce8c-b252-4521-f19f-d3d5744fc379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "cell_type": "code",
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "ite =1 \n",
        "global_loss = []\n",
        "for epoch in range(epochs) : \n",
        " \n",
        "    #hidden = encoder.initialize_hidden_state()\n",
        "    sys.stdout.write(\"Epoch: {} \\n\".format(epoch + 1))\n",
        "\n",
        "    start = time.time()\n",
        "    val_loss = 0\n",
        "    total_loss = 0\n",
        "    for (batch, (X, Y)) in enumerate(train_dataset): \n",
        "          loss = 0\n",
        "    \n",
        "          with tf.GradientTape() as tape:\n",
        "\n",
        "              enc_output,enc_hidden = encoder(X)\n",
        "              dec_hidden = enc_hidden\n",
        "              dec_input = tf.expand_dims(Y[:,0],axis=1)\n",
        "              #print(Y.shape)\n",
        "              for t in range(1,Y.shape[1]): \n",
        "\n",
        "                  dec_output,dec_hidden,_ = decoder(dec_input,dec_hidden,enc_output)\n",
        "\n",
        "                  loss += compute_loss(Y[:,t],dec_output)\n",
        "\n",
        "                  dec_input = tf.expand_dims(Y[:,t],axis=1)\n",
        "\n",
        "              batch_loss = loss/batch_size\n",
        "              total_loss += batch_loss\n",
        "\n",
        "              variables = encoder.variables + decoder.variables\n",
        "\n",
        "              gradients = tape.gradient(loss, variables)\n",
        "\n",
        "              optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "              if(batch%3==0):\n",
        "                sys.stdout.write(\"\\r{}/{} -- Loss {:.4f} -- lr:{} \".format(\n",
        "                                                              batch,N_BATCH,\n",
        "                                                              batch_loss.numpy(),optimizer._lr))\n",
        "                sys.stdout.flush()\n",
        "                \n",
        "                \n",
        "              #lr = cyclic_lr(0.003,0.01,ite,'triangular',8000,None)\n",
        "              global_loss.append(batch_loss)\n",
        "              learning_rate.assign(lr[ite])\n",
        "              ite = ite +1 \n",
        "              \n",
        " \n",
        "    for (batch, (X, Y)) in enumerate(val_dataset): \n",
        "              loss = 0\n",
        "    \n",
        "         \n",
        "\n",
        "              enc_output,enc_hidden = encoder(X)\n",
        "              dec_hidden = enc_hidden\n",
        "              dec_input = tf.expand_dims(Y[:,0],axis=1)\n",
        "              \n",
        "\n",
        "              for t in range(1,Y.shape[1]): \n",
        "\n",
        "                  dec_output,dec_hidden,_ = decoder(dec_input,dec_hidden,enc_output)\n",
        "\n",
        "                  loss += compute_loss(Y[:,t],dec_output)\n",
        "\n",
        "                  dec_input = tf.expand_dims(Y[:,t],axis=1)\n",
        "\n",
        "              batch_loss = loss/batch_size\n",
        "              val_loss += batch_loss\n",
        "\n",
        "                        \n",
        "    # saving (checkpoint) the model each epoch\n",
        "    if (epoch + 1) % 1== 0:\n",
        "      \n",
        "      !rm -r model.zip \n",
        "      !rm -r ./training_checkpoints\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      !zip -r -qq model.zip ./training_checkpoints \n",
        "      Drive().fileaction(['model.zip'],'up')\n",
        "      \n",
        "    \n",
        "    print(\"\")\n",
        "    sys.stdout.write(\"\\rLoss: {:.4f} -- Val_loss: {:.4f} \".format(\n",
        "                                                               total_loss / N_BATCH,val_loss/N_VAl))\n",
        "    sys.stdout.flush()\n",
        "   \n",
        "    print('\\nTime taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "    #!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \n",
            "849/849 -- Loss 0.0118 -- lr:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.29715e-05> Saved 'model.zip' to Drive\n",
            "\n",
            "Loss: 0.0853 -- Val_loss: 0.1350 \n",
            "Time taken for 1 epoch 970.571409702301 sec\n",
            "\n",
            "Epoch: 2 \n",
            "849/849 -- Loss 0.0115 -- lr:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.59465e-05> Saved 'model.zip' to Drive\n",
            "\n",
            "Loss: 0.0850 -- Val_loss: 0.1351 \n",
            "Time taken for 1 epoch 964.1758131980896 sec\n",
            "\n",
            "Epoch: 3 \n",
            "849/849 -- Loss 0.0115 -- lr:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.89215e-05> Saved 'model.zip' to Drive\n",
            "\n",
            "Loss: 0.0850 -- Val_loss: 0.1351 \n",
            "Time taken for 1 epoch 962.2085185050964 sec\n",
            "\n",
            "Epoch: 4 \n",
            "849/849 -- Loss 0.0115 -- lr:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.18965e-05> Saved 'model.zip' to Drive\n",
            "\n",
            "Loss: 0.0849 -- Val_loss: 0.1351 \n",
            "Time taken for 1 epoch 963.1241302490234 sec\n",
            "\n",
            "Epoch: 5 \n",
            "849/849 -- Loss 0.0114 -- lr:<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.48715e-05> Saved 'model.zip' to Drive\n",
            "\n",
            "Loss: 0.0849 -- Val_loss: 0.1352 \n",
            "Time taken for 1 epoch 965.8992600440979 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zfIT-E4cWekg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate\n"
      ]
    },
    {
      "metadata": {
        "id": "nEyFHwv3jhs8",
        "colab_type": "code",
        "outputId": "a4e6bb91-2ec9-4339-d39d-c54823093065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "def tokens_to_sentence(tokenizer,tokens): \n",
        "    sen =\"\" \n",
        "    for token in tokens: \n",
        "          word = tokenizer.index_word[token] \n",
        "       \n",
        "          if(word!=\"strr\"& word!='endd'): \n",
        "                sen += \" \"+ word\n",
        "    return sen        \n",
        "def compute_BLEU(seq,ref):\n",
        "    references = [x.strip().split() for x in ref]\n",
        "    #print(list(references))\n",
        "    hypothesis = seq\n",
        "    hypothesis = hypothesis.strip().split() \n",
        "    BLEUscore = nltk.translate.bleu_score.sentence_bleu(references, hypothesis,smoothing_function= nltk.translate.bleu_score.SmoothingFunction().method1) \n",
        "    return BLEUscore\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((data['eng_data_test'],data['french_data_test']))\n",
        "test_dataset = test_dataset.batch(1)\n",
        "\n",
        "french_tokenizer = data['french_tokenizer']\n",
        "eng_tokenizer = data['eng_tokenizer']\n",
        "\n",
        "sequence_length = 40\n",
        "out_seq_length = 40\n",
        "\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "bleu_scores = []\n",
        "for (batch, (X, Y)) in enumerate(test_dataset): \n",
        "                  enc_output,enc_hidden = encoder(X)\n",
        "                  dec_hidden = enc_hidden\n",
        "                  seq = \"strr\"\n",
        "                  source = \"\"\n",
        "                  target = \"\"\n",
        "                  tokens = french_tokenizer.texts_to_sequences([seq])\n",
        "                  pad_tokens = pad_sequences(tokens,sequence_length,padding='post')[:,0]\n",
        "                  dec_input = tf.expand_dims(pad_tokens,axis=1)\n",
        "\n",
        "                  for t in range(1,out_seq_length): \n",
        "\n",
        "                      \n",
        "                      dec_output,dec_hidden,_ = decoder(dec_input,dec_hidden,enc_output)\n",
        "\n",
        "                      top_indices = np.argsort(-dec_output,axis=-1)\n",
        "                      \n",
        "                      #dec_input = \n",
        "                      #print(np.asscalar(Y[:,t].numpy()))\n",
        "                      word = french_tokenizer.index_word[np.asscalar(top_indices[:,0])] \n",
        "                      if(word!=\"endd\"): seq += \" \" + word \n",
        "                      if(np.asscalar(X[:,t].numpy())!=0 ): \n",
        "                        source+= \" \"+ eng_tokenizer.index_word[np.asscalar(X[:,t].numpy())]\n",
        "                      if(np.asscalar(Y[:,t].numpy())!=0 ):   \n",
        "                        target+= \" \"+ french_tokenizer.index_word[np.asscalar(Y[:,t].numpy())]\n",
        "                      if(word!=\"endd\"): dec_input = tf.expand_dims(top_indices[:,0],axis=1)\n",
        "                  \n",
        "                  \n",
        "                  #print(\"source= \",source)    \n",
        "                  #print(\"target= \",target)\n",
        "                  #print(\"translated= \",seq[5:])\n",
        "                  bleu  = compute_BLEU(seq[5:]+\" endd\",[target])\n",
        "\n",
        "                  sys.stdout.write(\"\\rEvaluation Progress: {}/{} -- BLEU SCORE: {:.4f} \".format(\n",
        "                                                               batch,len(data['eng_data_test']),bleu))\n",
        "                  sys.stdout.flush()\n",
        "                  #print(\"BLEU SCORE =\",bleu)\n",
        "                  bleu_scores.append(bleu)\n",
        "\n",
        "                  #print(return_reference(source))\n",
        "print(\"\\nAverage Bleu Score:\" ,np.array(bleu_scores).mean())\n",
        "                  \n",
        "                  \n",
        "\n",
        "#print(french)                      \n",
        "#print(seq)                      \n",
        "\n",
        "\n",
        "                 \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Evaluation Progress: 13584/13585 -- BLEU SCORE: 0.0309 \n",
            "Average Bleu Score: 0.2663883949708836\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}